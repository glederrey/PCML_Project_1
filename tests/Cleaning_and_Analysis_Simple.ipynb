{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython import display\n",
    "# Import everything in the functions folder\n",
    "from functions.proj1_helpers import *\n",
    "from functions.clean_data import *\n",
    "from functions.least_squares import *\n",
    "from functions.split import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Analysis\n",
    "\n",
    "In this notebook, we are trying different way of cleaning the data and we're analysing the effect on the training/prediction. In order to analyze the effect, we will perform a least-square on 80% of the training data and test on the 20% remaining.\n",
    "\n",
    "But before doing that, we need to make sure that if there are some problem in the training data, there will be the same problems in the testing data.\n",
    "\n",
    "## Check Problems in Training and Testing data\n",
    "\n",
    "First, we load the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = 'data/train.csv' \n",
    "_, _, _, headers = load_data(DATA_TRAIN_PATH)\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "DATA_TEST_PATH = 'data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the percentage of NaNs for both training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variables: 30\n",
      "  train      test  \t Parameters\n",
      "0.152456 - 0.152204 \t DER_mass_MMC\n",
      "0.000000 - 0.000000 \t DER_mass_transverse_met_lep\n",
      "0.000000 - 0.000000 \t DER_mass_vis\n",
      "0.000000 - 0.000000 \t DER_pt_h\n",
      "0.709828 - 0.708851 \t DER_deltaeta_jet_jet\n",
      "0.709828 - 0.708851 \t DER_mass_jet_jet\n",
      "0.709828 - 0.708851 \t DER_prodeta_jet_jet\n",
      "0.000000 - 0.000000 \t DER_deltar_tau_lep\n",
      "0.000000 - 0.000000 \t DER_pt_tot\n",
      "0.000000 - 0.000000 \t DER_sum_pt\n",
      "0.000000 - 0.000000 \t DER_pt_ratio_lep_tau\n",
      "0.000000 - 0.000000 \t DER_met_phi_centrality\n",
      "0.709828 - 0.708851 \t DER_lep_eta_centrality\n",
      "0.000000 - 0.000000 \t PRI_tau_pt\n",
      "0.000000 - 0.000000 \t PRI_tau_eta\n",
      "0.000000 - 0.000000 \t PRI_tau_phi\n",
      "0.000000 - 0.000000 \t PRI_lep_pt\n",
      "0.000000 - 0.000000 \t PRI_lep_eta\n",
      "0.000000 - 0.000000 \t PRI_lep_phi\n",
      "0.000000 - 0.000000 \t PRI_met\n",
      "0.000000 - 0.000000 \t PRI_met_phi\n",
      "0.000000 - 0.000000 \t PRI_met_sumet\n",
      "0.000000 - 0.000000 \t PRI_jet_num\n",
      "0.399652 - 0.400286 \t PRI_jet_leading_pt\n",
      "0.399652 - 0.400286 \t PRI_jet_leading_eta\n",
      "0.399652 - 0.400286 \t PRI_jet_leading_phi\n",
      "0.709828 - 0.708851 \t PRI_jet_subleading_pt\n",
      "0.709828 - 0.708851 \t PRI_jet_subleading_eta\n",
      "0.709828 - 0.708851 \t PRI_jet_subleading_phi\n",
      "0.000000 - 0.000000 \t PRI_jet_all_pt\n"
     ]
    }
   ],
   "source": [
    "nan_train = perc_nan(tX)\n",
    "nan_test = perc_nan(tX_test)\n",
    "print(\"Number of variables: %i\"%(len(tX[0])))\n",
    "\n",
    "print(\"  train      test  \\t Parameters\")\n",
    "for i in range(len(nan_train)):\n",
    "    print(\"%f - %f \\t %s\"%(nan_train[i], nan_test[i], headers[i+2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that everytime the percentage of NaNs for a given parameter is higher than 0 in the training data, it will also be higher than 0 in the test data. Also the percentage is always close. Therefore, we can say that that if we perform some operation to clean the data for the training data, we can do it for the test data as well.\n",
    "\n",
    "## Benchmark\n",
    "\n",
    "Let's apply the least on the training data as they are right now. It will give us a benchmark to see if we can perform better just be removing problems. \n",
    "\n",
    "First, we split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratio = 0.8\n",
    "x_train, y_train, x_test, y_test = split_non_random(tX, y, ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can do the Least Square on the training data and apply the weights to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.823999\n",
      "Good prediction: 37229/50000 (74.458000%)\n",
      "Wrong prediction: 12771/50000 (25.542000%)\n"
     ]
    }
   ],
   "source": [
    "loss, w_star = least_square(y_train, x_train)\n",
    "print(\"Loss = %f\"%(loss))\n",
    "prediction(y_test, x_test, w_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Testing\n",
    "\n",
    "## Remove columns\n",
    "\n",
    "The first thing we can clean is the columns with a high percentage of NaNs. Let's try to remove all the columns with around 70% of NaN.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variables: 23\n"
     ]
    }
   ],
   "source": [
    "tX_without_nan, _, _ = delete_column_nan(nan_train, tX, headers, threshold = 0.65)\n",
    "print(\"Number of variables: %i\"%(len(tX_without_nan[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can redo the test we did with the benchmark and check if it becomes better or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.845954\n",
      "Good prediction: 36303/50000 (72.606000%)\n",
      "Wrong prediction: 13697/50000 (27.394000%)\n"
     ]
    }
   ],
   "source": [
    "ratio = 0.8\n",
    "x_train, y_train, x_test, y_test = split_non_random(tX_without_nan, y, ratio)\n",
    "# Do the training with LS and prediction\n",
    "loss, w_star = least_square(y_train, x_train)\n",
    "print(\"Loss = %f\"%(loss))\n",
    "prediction(y_test, x_test, w_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a really interesting result. If we remove the columns with the NaN values, the prediction becomes worse. Let's try again, but we remove all columns with NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variables: 19\n",
      "Loss = 0.851050\n",
      "Good prediction: 36161/50000 (72.322000%)\n",
      "Wrong prediction: 13839/50000 (27.678000%)\n"
     ]
    }
   ],
   "source": [
    "tX_without_nan, _, _ = delete_column_nan(nan_train, tX, headers, threshold = 0.01)\n",
    "print(\"Number of variables: %i\"%(len(tX_without_nan[0])))\n",
    "\n",
    "ratio = 0.8\n",
    "x_train, y_train, x_test, y_test = split_non_random(tX_without_nan, y, ratio)\n",
    "# Do the training with LS and prediction\n",
    "loss, w_star = least_square(y_train, x_train)\n",
    "print(\"Loss = %f\"%(loss))\n",
    "prediction(y_test, x_test, w_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, **removing columns with NaNs is a bad idea**. So, we'll try now to replace the NaNs by the mean of the non-NaN values.\n",
    "\n",
    "## mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variables: 30\n",
      "Loss = 0.829123\n",
      "Good prediction: 36988/50000 (73.976000%)\n",
      "Wrong prediction: 13012/50000 (26.024000%)\n"
     ]
    }
   ],
   "source": [
    "tX_replaced_mean = replace_by_mean(tX, nan_train)\n",
    "print(\"Number of variables: %i\"%(len(tX_replaced[0])))\n",
    "\n",
    "\n",
    "ratio = 0.8\n",
    "x_train, y_train, x_test, y_test = split_non_random(tX_replaced_mean, y, ratio)\n",
    "# Do the training with LS and prediction\n",
    "loss, w_star = least_square(y_train, x_train)\n",
    "print(\"Loss = %f\"%(loss))\n",
    "prediction(y_test, x_test, w_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## median\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variables: 30\n",
      "Loss = 0.829123\n",
      "Good prediction: 36988/50000 (73.976000%)\n",
      "Wrong prediction: 13012/50000 (26.024000%)\n"
     ]
    }
   ],
   "source": [
    "tX_replaced_median = replace_by_median(tX, nan_train)\n",
    "print(\"Number of variables: %i\"%(len(tX_replaced[0])))\n",
    "\n",
    "\n",
    "ratio = 0.8\n",
    "x_train, y_train, x_test, y_test = split_non_random(tX_replaced_median, y, ratio)\n",
    "# Do the training with LS and prediction\n",
    "loss, w_star = least_square(y_train, x_train)\n",
    "print(\"Loss = %f\"%(loss))\n",
    "prediction(y_test, x_test, w_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Apparently, keeping the data as they are is the **best option**. So, we won't clean the data (for the moment). But this is only valid with the Least_Square. So, le'ts try to keep the data with the -999 replaced by the median and then we'll apply the Ridge Regression on it in another notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_data('data/train_cleaned.csv', y, tX_replaced_median, ids, headers, 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same procedure for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tX_replaced_median_test = replace_by_median(tX_test, nan_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_data('data/test_cleaned.csv', _, tX_replaced_median_test, ids_test, headers, 'test')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
